\documentclass{report}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[inline]{enumitem}  % enumerate
\usepackage{listings}
\usepackage{tikz}
%\usepackage{forest}

\usetikzlibrary{patterns}
\usetikzlibrary{positioning}
\usetikzlibrary{graphs,graphdrawing}
\usegdlibrary{trees}

\lstset{
	basicstyle=\ttfamily,
	columns=fullflexible,  % thanks https://tex.stackexchange.com/a/172705
	keepspaces=true,
}

%\forestset{
%	default preamble={
%		for tree={circle,draw},
%		/tikz/every node/.append style={font=\footnotesize},
%	}
%}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\begin{document}

\chapter{Introduction to algorithm design}

n/a

\chapter{Algorithm analysis}

\section*{Notes}
The dominance pecking order:
\begin{gather*}
	n! \gg c^n \gg n^3 \gg n^2 \gg n^{1+\epsilon} \gg n\log n \gg n \gg \sqrt{n} \gg \\
	\log^2 n \gg \log n \gg \log n/\log\log n \gg \log\log n \gg \alpha(n) \gg 1
\end{gather*}

\section*{Solutions}
\paragraph{2-10}
\begin{enumerate}[label=(\alph*)]
	\item $f(n) = (n^2 - n)/2,\ g(n) = 6n.$

		Is $f(n) = O(g(n))$? If so, there is $c$ such that $f(n) \le cg(n)$
		for sufficiently large $n$.
		\begin{equation*}
			\frac{1}{2}\left(n^2 - n\right) \le 6n
				\ \rightarrow\ n^2 - n \le 12n
				\ \rightarrow\ n(n-1) \le 12n
		\end{equation*}
		Suppose there is such a $c$, then
		\begin{equation*}
			n(n-1) \le 12cn\ \rightarrow\ n-1 \le 12c
		\end{equation*}
		Clearly we can always find $n$ such that this inequality won't hold,
		so $f(n) \ne O(g(n))$.

		Is $g(n) = O(f(n))$? If so, there is $c$ such that $g(n) \le cf(n)$
		for sufficiently large $n$.
		\begin{equation*}
			6n \le \frac{1}{2}\left(n^2 - n\right)
			\ \rightarrow\ 12n \le n^2 - n = n(n-1)
			\ \rightarrow\ 12 \le n - 1
			\ \rightarrow\ 13 \le n.
		\end{equation*}
		So with $c = 1$ the inequality will hold for $n_0 \ge 13$, and $g(n) = O(f(n))$.

	\item $f(n) = n + 2\sqrt{n},\ g(n) = n^2.$

		$f(n) = O(g(n)) \Leftrightarrow f(n) \le cg(n)$ for sufficiently large $n$.
		\begin{gather*}
			n + 2\sqrt{n} \le cn^2,\ \text{with}\ c = 1, \\
			n + 2\sqrt{n} \le 2n\ \text{for $n > 4$}, \\
			2n \le n^2\ \text{so}\ f(n) = O(g(n)).
		\end{gather*}

		$g(n) = O(f(n)) \Leftrightarrow g(n) \le cf(n)$ for sufficiently large $n$. But
		this asks to find $c$ such that $n^2 \le c\left(n + 2\sqrt{n}\right)$; since
		ultimately $n^2 \gg n$, $g(n) \ne O(f(n))$.

	\item $f(n) = n\log n,\ g(n) = n\sqrt{n}.$
		\begin{gather*}
			f(n) = O(g(n)) \Leftrightarrow n\log n \le cn\sqrt{n},\ \text{with $c=1,$} \\ 
			\rightarrow\ \log n \le \sqrt{n/2},
		\end{gather*}
		since $\sqrt{n} \gg \log n,\ f(n) = O(g(n)).$

		By the same argument, $g(n) \ne O(f(n)).$

	\item $f(n) = n + \log n,\ g(n) = \sqrt{n}\ \rightarrow\ n + \log n \le c\sqrt{n}$, and
		since $n \gg \sqrt{n}$, any constant factor will be dominated by the linear term, so
		$f(n) \ne O(g(n)).$ Conversely and by the same argument, $g(n) = O(f(n)).$

	\item $f(n) = 2\left(\log n\right)^2,\ g(n) = \log n + 1.$ Note that
		$2\left(\log n\right)^2 = 2\log^2 n$, and $\log^2 n \gg \log n,$ so $g(n) = O(f(n))$
		and $f(n) \ne O(g(n)).$

	\item $f(n) = 4n\log n + n,\ g(n) = \left(n^2 - n\right)/2.$ We know that
		$n\log n \gg n,$ so we can consider just this term from $f(n).$ But ultimately the
		quadratic term in $g(n)$ dominates so $f(n) = O(g(n)).$
\end{enumerate}


\paragraph{2-11}
\begin{enumerate}[label=(\alph*)]
	\item $f(n) = 3n^2,\ g(n) = n^2.$

		With $c = 3,\ f(n) \le 3g(n)$ so $f(n) = O(g(n)).$

		$f(n) = \Omega(n) \Leftrightarrow cg(n) \le f(n)$ for sufficiently large $n$. For
		$c = 1$ the inequality holds, so $f(n) = \Omega(g(n))$ and $f(n) = \Theta(g(n)).$

	\item $f(n) = 2n^4 - 3n^2 + 7,\ g(n) = n^5.$
	
		$n^5 \gg n^4$ so $f(n) = O(g(n))$ and $f(n) \ne \Omega(g(n)).$

	\item $f(n) = \log n,\ g(n) = \log n + \frac{1}{n}.$

		$\lim_{n\rightarrow\infty} \frac{1}{n} = 0,$ so as $n\rightarrow\infty$,
		$f(n) - g(n) = 0$. So no function dominates the other. Thus, $f(n) = \Theta(g(n)).$

	\item $f(n) = 2^{k\log n},\ g(n) = n^k.$
		\begin{gather*}
			f(n) = O(g(n)) \Leftrightarrow f(n) \le cg(n) \\
			\rightarrow\ 2^{k\log n} \le cn^k;\ \text{taking logarithms,} \\
			\rightarrow\ \log\left(2^{k\log n}\right) \le \log\left(cn^k\right)
				= \log c + \log n^k \\
			\rightarrow\ k\log n\log 2 \le \log c + k\log n.
		\end{gather*}
		Ignoring constant terms and multiplicative constants, we are left with
		$\log n \le \log n$, so $f(n) = \Theta(g(n)).$

	\item $f(n) = 2^n,\ g(n) = 2^{2n}.$

		$2^n \le c2^{2n}$ clearly holds for $c = 1$, so $f(n) = O(g(n)).$

		$c2^{2n} \le 2^n$? Well, $2^{2n} = 2^2\cdot2^n = 4\cdot2^n,$ so
		$4c2^n \le 2^n$ is satisfied with $c = 1/4.$ So $f(n) = \Omega(g(n))$ and
		finally, $f(n) = \Theta(g(n)).$
\end{enumerate}

\paragraph{2-12}$n^3 - 3n^2 -n + 1 = \Theta(n^3).$

Note that $0 \le 3n^2 + n \rightarrow n^3 \le n^3 + 3n^2 + n \rightarrow n^3 - 3n^2 - n \le n^3$. Thus $f(n) = O(n^3)$.

Now $cn^3 \le n^3 - 3n^2 - n + 1.$ Consider $c = 1/2$, then
\begin{align*}
	n^3/2 &\le n^3 - 3n^2 - n + 1 \\
	-n^3/2 &\le -3n^2 - n + 1 \\
	-n^3 &\le -6n^2 - 2n + 2 \\
	n^3 &\ge 6n^2 + 2n - 2. 
\end{align*}
This holds for $n_0 \ge 7$, so $f(n) = \Omega(n^3)$ and finally $f(n) = \Theta(n^3)$.

\paragraph{2-13}$f(n) = n^2 = O\left(2^n\right) \Leftrightarrow f(n) \le c2^n,$ after some $n_0$. For $c = 1$,
\begin{align*}
	n^2 &\le 2^n \\
	\log(n^2) &\le \log(2^n) \\
	2 \log n &\le n\log 2 \\
	\log n &\le kn,\ k = \frac{\log 2}{2}
\end{align*}

Since $\log n \ll n$ this inequality holds for large enough $n$, and $n^2 = O(2^n)$.

\paragraph{2-14} $\Theta(n^2) = \Theta(n^2+1)$? This is to say whether both classes are the same. We can show this by assuming we have $f(n) = \Theta(n^2)$, and proving that $f(n) = \Theta(n^2+1)$, and likewise with the other assumption.

First, $f(n) = \Theta(n^2) \Rightarrow f(n) = O(n^2)$ and $f(n) = \Omega(n^2)$. Is $f(n) = O(n^2+1)$? This would mean $f(n) \le c\left(n^2+1\right)$, for $n > n_0$. Since $f(n) = O(n^2)$ there is $c_0$ such that $f(n) \le c_0 n^2$, but $c_0 n^2 \le c_0\left(n^2 + 1\right)$ so with the same $c_0$ we see that $f(n) = O(n^2+1)$.

Now we want to show that $f(n) = \Omega(n^2+1)$, that is, $c\left(n^2 + 1\right) \le f(n)$ for $n > n_0$. Since $f(n) = \Omega(n^2)$ there are $c_1, n_1$ such that $c_1 n^2 \le f(n)$, for $n > n_1$. In particular this holds for $n+1 > n_1$, so
\begin{equation*}
	c_1 n^2 < c_1 (n+1)^2 \le f(n), \quad n > n_1.
\end{equation*}
But note that $n^2 + 1 \le (n+1)^2$, so for $n_0 = n_1 + 1$ above, we have that $c_1\left(n^2 + 1\right) \le c_1 (n+1)^2 \le f(n)$ for $n > n_0$. So $f(n) = \Omega(n^2+1)$ and thus $f(n) = \Theta(n^2 + 1)$.

\vskip 1em

Now we need to show that $f(n) = \Theta(n^2+1) \Rightarrow f(n) = \Theta(n^2)$.

$f(n) = \Omega(n^2) \Leftrightarrow c n^2 \le f(n)$ for $n > n_0$. We know that $f(n) = \Omega(n^2+1)$ so there is a $c_1$ such that $c_1\left(n^2+1\right) \le f(n)$ for $n > n_1$; since $n^2 < n^2 + 1$, by letting $c = c_1$ we see that $c_1 n^2 < c_1 \left(n^2+1\right) \le f(n)$, so $f(n) = \Omega(n^2)$.

$f(n) = O(n^2) \Leftrightarrow f(n) \le c n^2$ for $n > n_0$. Since $f(n) = O(n^2 + 1)$ we know there are $c_1$ and $n_1$ such that $f(n) \le c_1\left(n^2+1\right)$ for $n \ge n_1$; in particular, $f(n) \le c_1\left(n_1^2+1\right)$. Then let $c = c_1\left(n_1^2+1\right)$, then $c n^2 > c_1\left(n_1^2+1\right) \ge f(n)$, for $n > n_1$. Thus $f(n) = O(n^2)$ and $f(n) = \Theta(n^2)$.

\vskip 1em

This shows that if a function is $\Theta(n^2)$ then it must be $\Theta(n^2+1)$ and viceversa---that is, both classes are the same.

\paragraph{2-17}
\begin{enumerate}[label=\alph*)]
	\item $f(n) = n^2 + n + 1,\ g(n) = 2n^3$. We want to find $c > 0$ such that $f(n) \le c g(n)$ for $n > 1$. $f(2) = 7;\ g(2) = 16 \rightarrow c = 1$.
	\item $f(n) = n\sqrt{n},\ g(n) = n^2$. $n\sqrt{n} < n^2 \rightarrow 2n^2 > n\sqrt{n} + n^2 \rightarrow c = 2$.
	\item $f(n) = n^2 - n + 1,\ g(n) = n^2/2$. $n^2 - n + 1 < \frac{c}{2}n^2 \rightarrow$ if $n = 2,\ f(2) = 3$, and $g(2) = 2$. For $c=2,\ n^2 - n + 1 < n^2. (f(2) = 3, 2g(2) = 4)$.
\end{enumerate}

\paragraph{2-18} Let $f_1(n) = O(g_1(n)),\ f_2(n) = O(g_2(n))$. Show that $f_1(n) + f_2(n) = O(g_1(n) + g_2(n))$.

\begin{proof}There are $c_1,\ n_1$ such that $f_1(n) \le c_1 g_1(n)$ for $n > n_1$, and $c_2,\ n_2$ such that $f_2(n) \le c_2 g_2(n)$ for $n > n_2$. Let $c = \max(c_1, c_2),\ n_0 = \max(n_1, n_2)$. Then $f_1(n) < c g_1(n)$, and $f_2(n) < c g_2(n)$, which implies that $f_1(n) + f_2(n) < c g_1(n) + c g_2(n) = c(g_1(n) + g_2(n))$ for $n > n_0$.
\end{proof}

\paragraph{2-19} Let $f_1(n) = \Omega(g_1(n)),\ f_2(n) = \Omega(g_2(n))$. Then there are $c_1, n_1$ such that $f_1(n) \ge c_1 g_1(n)$ for $n > n_1$, and $c_2, n_2$ such that $f_2(n) \ge c_2 g_2(n)$ for $n > n_2$. Let $n_0 = \max(n_1, n_2)$ and let $c_0 = \min(c_1, c_2)$. Then $c g_1(n) \le c_1 g_1(n)$ and also $c g_2(n) \le c_2 g_2(n)$. These inequalities imply that
\begin{gather*}
	c g_1(n) \le f_1(n),\ n > n_0, \\
	c g_2(n) \le f_2(n),\ n > n_0.
\end{gather*}
Thus $c(g_1(n) + g_2(n)) \le f_1(n) + f_2(n),\ n > n_0$, and $f_1(n) + f_2(n) = \Omega(g_1(n) + g_2(n))$.

\paragraph{2-20} Let $f_1(n) = O(g_1(n))$ and $f_2(n) = O(g_2(n))$. Then there are $c_1, n_1, c_2, n_2$ such that $f_1(n) \le c_1 g_1(n)$ for $n > n_1$, and $f_2(n) \le c_2 g_2(n)$ for $n > n_2$. Let $c = \max(c_1, c_2)$ and $n_0 = \max(n_1, n_2)$. Then $f_1(n) \le c g_1(n)$ and $f_2(n) \le c g_2(n)$ for $n > n_0$, which implies that $f_1(n)\cdot f_2(n) \le c(g_1(n)\cdot g_2(n))$. Thus $f_1(n)\cdot f_2(n) = O(g_1(n)\cdot g_2(n))$.

\paragraph{2-21} We are to prove that $p(n) = a_k n^k + \ldots + a_0 = O(n^k)$, for $k \ge 0$ and arbitrary real coefficients $a_i$. This is to say that we can find a $c > 0$ and $n_0$ such that $p(n) \le c n^k$, for all $n > n_0$.

\begin{proof}We know that after some $n$, $a_k n^k > p(n) - a_k n^k$, that is to say, the leading order term will come to dominate. To see this, note that
\begin{equation*}\lim_{n\to\infty}\frac{n^m}{n^q} = 0 \Leftrightarrow q > m,\end{equation*}
is to say that $n^q \gg n^m$.

Now take $c = \max(a_0,\ldots, a_k)$. Then clearly $c n^k + \ldots + c \ge a_k n^k + \ldots + a_0 = p(n)$. But because of the same argument as above, there is some $n_0$ after which $c n^k > c n^{k-1} + \ldots + c$. Then it follows that $p(n) \le c n^k$ for $n > n_0$, which means $p(n) = O(n^k)$.
\end{proof}

\paragraph{2-22} Let $a, b \in \R$, with $b > 0$. Show that $(n+a)^b = \Theta(n^b)$.

\begin{proof}
To prove that $(n+a)^b = O(n^b)$ we need to find $c_0$ such that $c_0 n^b \ge (n+a)^b$ for sufficiently large $n$. If $a=0,\ (n+a)^b = n^b$ and with $c=1$ the inequality holds. If $a<0, (n+a)^b \le n^b$, so
\begin{equation*}
	\frac{(n+a)^b}{n^b} = \left(\frac{n+a}{n}\right)^b < 1.
\end{equation*}
If $a>0$ we need to see that $\left(1 + \frac{a}{n}\right)^b$ is bounded. (To see why, $(n+a)^b \le c n^b \rightarrow (n+a/n)^b \le c \rightarrow (1 + a/n)^b \le c$).

Note that $\frac{a}{n}$ can get arbitrarily close to 0 for large enough $n$, so the entire expression tends to 1 as $n\to\infty$. Thus for, say, $c=2$ it is possible to find $n_0$ large enough such that $(n+a)^b \le 2n^b$. So $(n+a)^b = O(n^b)$.

\vskip 0.5em

To prove that $(n+a)^b = \Omega(n^b)$, by the same argument, $c n^b \le (n+a)^b \rightarrow c \le (n+a/n)^b = (1 + a/n)^b$. For $a\ne0$ the expression $1+a/n$ tends to 1, either ``from below'' or ``from above''---at any rate, by taking some $c<1$, say, $1/2$, it will be possible to find $n$ large enough that the inequality will hold. Thus $(n+a)^b = \Omega(n^b)$, and finally $(n+a)^b = \Theta(n^b)$.
\end{proof}

\paragraph{2-27}
\begin{enumerate}[label=(\alph*)] 
	\item $f(n) = o(g(n))$ and $f(n)\ne\Theta(g(n))$. If $f(n) = o(g(n))$ then $g(n)\gg f(n)$---they are on different classes. But $f(n)\ne\Theta(g(n))$ implies that either $f(n)\ne\Omega(g(n))$ or $f(n)\ne O(g(n))$. Take $f(n)=n,\ g(n)=n^2$. Clearly $f(n)\ne\Omega(g(n))$ but $g(n)\gg f(n)$ so $f(n)=o(g(n))$.
	\item $f(n) = \Theta(g(n)),\ f(n) = o(g(n))$. If $f(n)=\Theta(g(n))$ then $f(n)=O(g(n))$ and $f(n)=\Omega(g(n))$, that is, they are in the same class. So it's not possible for both $f(n)=\Omega(g(n))$ and $f(n)=o(g(n))$ to hold simultaneously.
	\item $f(n)=\Theta(g(n))$ and $f(n)\ne O(g(n))$. None, by definition.
	\item $f(n)=\Omega(g(n))$ and $f(n)\ne O(g(n))$. $f(n)=n^2,\ g(n)=n$.
\end{enumerate}

\paragraph{2-29}
\begin{enumerate}[label=(\alph*)]
	\item $f(n) = n^2 + 3n + 4,\ g(n) = 6n + 7 \rightarrow f(n) = \Omega(g(n))$.
	\item $f(n) = n\sqrt{n},\ g(n)=n^2-n \rightarrow f(n) = \Omega(g(n))$.
	\item $f(n) = 2^n-n^2,\ g(n)=n^4+n^2 \rightarrow f(n) = \Omega(g(n))$.
\end{enumerate}

\paragraph{2-30}
\begin{enumerate}[label=(\alph*)]
	\item Yes---$O(n^2)$ worst case time doesn't necessarily mean it ever takes $n^2$ steps on any input. If the algorithm was $O(n)$ worst case, it would still be $O(n^2)$.
	\item Yes---$O(n^2)$ only talks about an upper bound.
	\item Yes. It could be that the best case time complexity of the algorithm is $O(n)$.
	\item No. Some inputs will trigger worst case behavior so will necessarily be $\Theta(n^2)$.
	\item Yes. We ignore multiplicative constants and terms of lower degree.
\end{enumerate}

\paragraph{2-31}
\begin{enumerate}[label=(\alph*)]
	\item Is $3^n = O(2^n)$? Only if there is $c$ such that $3^n \le c 2^n$ for sufficiently large $n$. But if there was such $c$, then $(3/2)^n \le c$, and since $3/2 > 1,\ (3/2)^n\to\infty$ as $n\to\infty$, thus $3^n \ne O(2^n)$.
	\item Is $\log 3^n = O(\log 2^n)$? Only if there is $c$ such that $\log 3^n \le c\log 2^n$ for sufficiently large $n$. Note that $\log 3^n = n\log 3$, and $c\log 2^n = cn\log 2$. So we want $c$ such that $n\log 3 \le cn\log2 \rightarrow \log 3 \le c\log2 \rightarrow \frac{\log 3}{\log 2} \le c$. The answer is yes, $\log 3^n = O(\log 2^n)$.
	\item Is $3^n = \Omega(2^n)$? Only if there is $c$ such that $c 2^n \le 3^n$ for large enough $n$. Trivially visible for $c=1$.
	\item Is $\log 3^n = \Omega(\log 2^n)$? Only if there exists $c$ such that $c\log 2^n \le \log 3^n$. Since $\log$ is monotonically increasing and $2^n \le 3^n$, also trivially visible for $c=1$.
\end{enumerate}

\paragraph{2-34}
\begin{enumerate}[label=(\alph*)]
	\item $\sum_1^n 3^i = \Theta(3^{n-1})$? This would imply both $\Omega(3^{n-1})$ and $O(3^{n-1})$. For the first we need $c$ such that $c 3^{n-1} \le \sum_1^n 3^i = 3^n + \ldots + 3$. Well, for $c=1$ this inequality holds.
	
	Now for $O(3^{n-1})$ we need $c$ such that $\sum_1^n 3^i \le c 3^{n-1}$. Let $c=9$, then $c 3^{n-1} = 9\cdot 3^{n-1} = 3^{n+1} \ge \sum_1^n 3^i$ for large enough $n$.
	
	Thus the answer is true.
	
	\item $\sum_1^n 3^i = \Theta(3^n)$? True.
	\item $\sum_1^n 3^i = \Theta(3^{n+1})$? True.
	
	For $\Omega(3^{n+1})$: $c 3^{n+1} \le \sum_1^n 3^i = 3^n + \ldots + 3$. Let $c=1/9$, then $c 3^{n+1} = (1/9)\cdot9\cdot3^{n-1} = 3^{n-1} \le \sum_1^n 3^i$ as stated before.
	
	For $O(3^{n+1})$: $\sum_1^n 3^i \le c 3^{n+1}$ for $c=1$.
\end{enumerate}

\paragraph{2-35}
\begin{enumerate}[label=(\alph*)]
	\item $g(n) = 4^n$
	\item $g(n) = n\log n$
	\item $g(n) = \log^{10} n$
	\item $g(n) = n^{100}$
\end{enumerate}

% TODO draw the triangle here
\paragraph{2-37} Each number in row $n-1$ is added exactly three times into row $n$. So if the value of the sum of row $n-1$ is $T$, then the sum of the values of row $n$ is $3T$. This suggests a recurrence
\begin{align*}
	T_1 &= 1 = 3^0, \\
	T_n &= 3\,T_{n-1},\quad n>1.
\end{align*}

The recurrence seems to hold, at least up to $n=5$. This suggests a closed form $t(n) = 3^{n-1},\ n \ge 1$. Base case $n=1$, clearly $t(1) = 3^0 = 1 = T_1$. Now assume this is true up to $n$, then $t(n+1) = 3^{n+1-1} = 3^n = 3\cdot3^{n-1} = 3t(n) = 3\,T_{n-1} = T_n$.

\paragraph{2-39} $\sum^{n+1} i = \sum^n i + (n+1) = n(n+1)/2 + n + 1$. Traverse the array adding up all the numbers and subtract the result from $\sum^{n+1} i$. The difference is the missing element.

\paragraph{2-40} The fragment:
\begin{lstlisting}
for i=1 to n do
  for j=i to 2*i do
    output ``foobar''
\end{lstlisting}
\begin{enumerate}[label=\alph*.]
	\item $T(n) = \sum_{i=1}^n \sum_{j=1}^{2i} 1$. Let's look at the inner sum. There are $i$ elements between $i$ and $2i$, so
	\item $\sum_i^{2i} 1 = \sum_1^i 1 = i$. Then $T(n) = \sum_{i=1}^n i = n(n+1)/2$.
\end{enumerate}

\paragraph{2-41} The fragment:
\begin{lstlisting}
for i=1 to n/2 do
  for j=i to n-i do
    for k=1 to j do
      output ``foobar''
\end{lstlisting}
\begin{enumerate}[label=\alph*.]
	\item $T(n) = \sum_{i=1}^{n/2} \sum_{j=i}^{n-i} \sum_{k=1}^j 1$.
	\item $\sum_{k=1}^j 1 = j \rightarrow T(n) = \sum_{i=1}^{n/2} \sum_{j=i}^{n-1} j$.
	
	Now consider $\sum_{j=i}^{n-i} j$. When $i = 1$, $j$ goes from 1 to $n-1$; for $i=2$, $j$ goes from 2 to $n-2$. So for some given $i$, this sum has $n-i-i = n-2i$ terms, from $i$ to $n-i$. We can add the numbers from 1 to $n-i$, $\sum_1^{n-i} i = \frac{1}{2}(n-i)(n-i+1)$, and take from that the sum from 1 to $i-1$, $\sum_{k=1}^{i-1} k = \frac{1}{2}(i-1)(i-1+1) = \frac{i(i-1)}{2}$. Then the sum $\sum_{j=i}^{n-i} j = \sum_{k=1}^{n-i} k - \sum_{k=1}^{i-1} k = \frac{n^2+n}{2}$.
	
	Back to $T(n)$. We now have the outer sum $T(n) = \sum_{i=1}^{n/2} \frac{n^2+n}{2} = (n/2)(\frac{n^2+n}{2}) = \frac{1}{4}(n^3 + n^2)$.
\end{enumerate}

\paragraph{2-42} Suppose we have two $n$-digit numbers in base $b$,
\begin{align*}
	x &= b^{n-1} x_{n-1} + b^{n-2} x_{n-2} + \ldots + b x_1 + b^0 x_0 \\
	y &= b^{n-1} y_{n-1} + \ldots + b^0 y_0.
\end{align*}
Each number is represented in base $b$ as the concatenation of the digits $x_i$, as in $x = \text{``}x_{n-1}x_{n-2}\ldots x_0\text{''}$. Their product will be $xy = (b^{n-1}x_{n-1} + \ldots + x_0)(b^{n-1} y_{n-1} + \ldots + y_0)$ and the highest order term of this product will be $b^{n-1} x_{n-1} b^{n-1} y_{n-1} = (b^{n-1})^2 x_{n-1}y_{n-1}$. Every other term in this product will be of lower order, so the complexity will be driven by the number $b^{2n-2}$.

We could say that $b^{2n-2} xy$ takes $O(1)$ for $xy$, which is then added to itself $b^{2n-2}$ times. Then the complexity of multiplying two numbers of $n$ digits in base $b$ is $O(b^{2n-2})$. Note that there are a lot of hidden multiplications in $b^{2n-2} = b\cdot b^{2n-3}$. If $b$ is one digit then it takes $O(1)$, but for $b=10$, well, I don't know.

\paragraph{2-44}
\begin{enumerate}[label=(\alph*)]
	\item By definition $a^{\log_a x} = x$. So $xy = a^{\log_a x} a^{\log_a y} = a^{\log_a x + \log_a y}$. Taking logarithms, $\log_a xy = \log_a a^{\log_a x + \log_a y} = \log_a x + \log_a y.$
	\item Consider $a^{y\log_a x} = \left(a^{\log_a x}\right)^y = x^y.$ Then taking logarithms, $\log_a x^y = \log_a a^{y\log_a x} = y\log_a x$.
	\item Consider $\log_a x\, \log_b a \rightarrow b^{\log_a x \log_b a} = \left(b^{\log_b a}\right)^{\log_a x} = a^{\log_a x} = x$. Taking logarithms, $\log_b x = \log_b b^{\log_a x \log_b a} = \log_a x\,\log_b a$. Equivalently, $\log_a x = \frac{\log_b x}{\log_b a}$.
	\item $\log_b y = w \rightarrow y = b^w$, so $y^{\log_b x} = \left(b^w\right)^{\log_b x} = \left(b^{\log_b x}\right)^w = x^w = x^{\log_b y}$.
\end{enumerate}


\chapter{Data structures}

\section*{Solutions}

\paragraph{3-5}
\begin{enumerate}[label=\alph*)]
	\item Suppose the array has size $2^n$. This underflow strategy has us release the
		top half of our allocated memory when we come down to $2^{n-1}$ items.
		
        \begin{center}
        \begin{tikzpicture}[scale=.46]
        	\foreach \x in {0,...,7} {
        		\ifnum \x < 4
        			\filldraw[pattern=north west lines] (\x,0) rectangle (\x+1, 1);
        		\else
        			\draw (\x,0) rectangle (\x+1,1);
        		\fi
        	}
        	\draw[->] (9,0.5) -- (10,0.5);
        	\foreach \x in {0,...,3}
        			\filldraw[pattern=north west lines] (\x+11,0) rectangle (\x+12, 1);
        \end{tikzpicture}
        \end{center}
        
        If our program now adds an item to the dynamic array the same space needs to be
        allocated that was just freed, potentially moving all $2^{n-1}$ items. Imagine
        this delete-one/add-one cycle repeats, such as may happen with a stack backed
        by this dynamic array. Each append-one operation copies the (now) lower half to
        a new location, taking linear time on the number of items.
        
	\item The problem with that underflow strategy is that both the ``grow'' and
		``shrink'' events are at the same threshold---when half full, shrink by half thus
		making it full again, but this guarantees the next append will trigger a ``grow''.
		If the two thresholds are dissociated, we will avoid this pathological behavior.
		For this, only shrink the array to half size when it is a fourth full.
		
        \begin{center}
        \begin{tikzpicture}[scale=.46]
        	\foreach \x in {0,...,7} {
        		\ifnum \x < 2
        			\filldraw[pattern=north west lines] (\x,0) rectangle (\x+1, 1);
        		\else
        			\draw (\x,0) rectangle (\x+1,1);
        		\fi
        	}
        	\draw[->] (9,0.5) -- (10,0.5);
        	\foreach \x in {0,...,3} {
        		\ifnum \x < 2
        			\filldraw[pattern=north west lines] (\x+11,0) rectangle (\x+12, 1);
        		\else
        			\draw (\x+11,0) rectangle (\x+12,1);
        		\fi
			}
        \end{tikzpicture}
        \end{center}
        
        We are then at the situation after a grow operation just duplicated the size of
        the array for us, thus amortizing the cost of grow/shrink operations.
\end{enumerate}

\paragraph{3-6}Skiena's fridge works as a stack, so unless he takes care of unstacking every food item regularly (thus emptying the fridge), that is bad news for the first items inserted.

One improvement might be to use a queue, whatever has been in there the longest is consumed first. However that is still a naive strategy---if an item expiring tomorrow is inserted after one that will expire in a year, I still risk the last item expiring.

The answer is a \emph{priority queue}. Prioritize the item that expires next. This ensures that items that have longer expiration dates wait the most.

\paragraph{3-7} The book says to keep a sentinel for the end of the list. There are three cases for delete: \begin{enumerate*}[label=\arabic*)]\item delete the first node; \item delete a node in the middle; \item delete the last node.\end{enumerate*}

\begin{enumerate}[label=\arabic*)]
	\item Deleting the head.
    \begin{center}
    \tikz[every rectangle node/.style={draw},
    	  every circle node/.style={draw}] {
    	\node[rectangle] (H) at (0,0) {H};
    	\node[circle] (1) at (1,0) {1};
    	\node[circle] (2) at (2,0) {2};
    	\node[rectangle] (T) at (3,0) {T};
    	\node[draw=white,scale=.7] (P) at (1,-0.7) {p};
    	\draw[->] (H) -- (1);
    	\draw[->] (1) -- (2);
    	\draw[->] (2) -- (T);
    	\draw[->] (P) -- (1);
    }
    \end{center}
    
    \lstinline!list.head == p!, point \lstinline!H! to \lstinline!p->next!, free \lstinline!p!
    
    \begin{center}
    \tikz[] {
    	\node[rectangle,draw] (H) at (0,0) {H};
    	\node[circle,draw=gray,dashed,gray] (1) at (1,0) {1};
    	\node[circle,draw] (2) at (2,0) {2};
    	\node[rectangle,draw] (T) at (3,0) {T};
    	\node[draw=white,scale=.7] (P) at (1,-0.7) {p};
    	\draw[->] (H) edge[bend left=45] (2);
    	\draw[->] (1) -- (2);
    	\draw[->] (2) -- (T);
    	\draw[->] (P) -- (1);
    }
    \end{center}

	\item Deleting a node in the middle of the list.
    \begin{center}
    \tikz[every rectangle node/.style={draw},
    	  every circle node/.style={draw}] {
    	\node[rectangle] (H) at (0,0) {H};
    	\node[circle] (0) at (1,0) {0};
    	\node[circle] (1) at (2,0) {1};
    	\node[circle] (2) at (3,0) {2};
    	\node[rectangle] (T) at (4,0) {T};
    	\node[draw=white,scale=.7] (P) at (2,-0.7) {p};
    	\draw[->] (H) -- (0);
	    \draw[->] (0) -- (1);
    	\draw[->] (1) -- (2);
    	\draw[->] (2) -- (T);
    	\draw[->] (P) -- (1);
    }
    \end{center}
    
    \lstinline;list.head != p;, \lstinline;p->next != tail;, overwrite \lstinline!p! with \lstinline!p->next!, then free \lstinline!p->next!.

	\begin{center}    
    \tikz[] {
    	\node[rectangle,draw] (H) at (0,0) {H};
		\node[circle,draw] (0) at (1,0) {0};
    	\node[circle,draw] (1) at (2,0) {2};
    	\node[circle,draw=gray,dashed,gray] (2) at (3,0) {2};
    	\node[rectangle,draw] (T) at (4,0) {T};
    	\node[draw=white,scale=.7] (P) at (2,-0.7) {p};
    	\draw[->] (1) edge[bend left=45] (T);
		\draw[->] (H) -- (0);
		\draw[->] (0) -- (1);
    	\draw[->] (2) -- (T);
    	\draw[->] (P) -- (1);
    }
    \end{center}
    
    \item Deleting the last node.
    
    \begin{center}
    \tikz[every rectangle node/.style={draw},
    	  every circle node/.style={draw}] {
    	\node[rectangle] (H) at (0,0) {H};
    	\node[circle] (1) at (1,0) {1};
    	\node[circle] (2) at (2,0) {2};
    	\node[rectangle] (T) at (3,0) {T};
    	\node[draw=white,scale=.7] (P) at (2,-0.7) {p};
    	\draw[->] (H) -- (1);
    	\draw[->] (1) -- (2);
    	\draw[->] (2) -- (T);
    	\draw[->] (P) -- (2);
    }
    \end{center}
    
    \lstinline!p->next == tail!, free \lstinline!tail! and make \lstinline!p! the new sentinel node.
    
	\begin{center}    
    \tikz[] {
    	\node[rectangle,draw] (H) at (0,0) {H};
		\node[circle,draw] (1) at (1,0) {1};
    	\node[rectangle,draw] (2) at (2,0) {T};
    	\node[rectangle,draw=gray,dashed,gray] (T) at (3,0) {T};
    	\node[draw=white,scale=.7] (P) at (2,-0.7) {p};
		\draw[->] (H) -- (1);
    	\draw[->] (1) -- (2);
    	\draw[->] (P) -- (2);
    }
    \end{center}
\end{enumerate}

\paragraph{3-8} The winning condition is for $n$ \lstinline!X! or \lstinline!O! to be placed in a row, column or diagonal. We can keep \lstinline!X! and \lstinline!O! counters for every row, column and diagonal. There are $n$ rows, $n$ columns and 2 diagonals, so we need $2n+2$ counters to keep state, which is $O(n)$ space as requested.

Consider a move to be represented as a tuple indicating the position played and the player, for example \lstinline!((1,1), X)!. Given a list of legal moves alternating players, execute each move by increasing the affected row and column, and diagonal counter for the player. The final move is a winning move if any of its affected counters are equal to $n$.

\begin{lstlisting}
enum Player {
    X = 0,
    O = 1,
}

struct Counter {
    counter: [u32; 2],
}

type Move = (usize, usize, Player);

struct TicTacToe {
    n: usize,
    rows: Vec<Counter>,
    cols: Vec<Counter>,
    diag: [Counter; 2],
    last_move: Option<Move>,
}
\end{lstlisting}

\paragraph{3-9} Each sequence of digits maps to multiple potential words---we have a one-to-many relation. Conversely, a word maps to a unique numeric representation with as many digits as the words as letters.

If multiple queries were necessary, we might justify transforming the dictionary into a hash map of numbers to lists of words. If words are on average $m$ characters long, and there are $n$ of them, we would pay $O(nm)$ complexity in traversing the entire dictionary and hashing every word.

Suppose we are to use the given dictionary only to collect all words that match our sequence of digits. If our given sequence of digits is $s$ digits long, we will have at most 4 possible characters associated with each digit, so there will be $4^s$ letter combinations to look up in the dictionary. So it looks like we are after all better off hashing the entire thing and querying directly with the sequence.

\begin{lstlisting}
fn hash_char(c: char) -> char {
    match c {
        'a' | 'b' | 'c' => '2',
        ...
        'w' | 'x' | 'y' | 'z' => '9',
        _ => unreachable!(),
    }
}

fn hash_dictionary() -> HashMap<String, Vec<String>> {
    let mut result = HashMap::new();
    for (key, word) in WORDS.iter().map(|w| {
        let hash: String = w.chars().map(hash_char).collect();
        (hash, w.to_string())
    }) {
        result.entry(key).or_default().push(word);
    }
    result
}
\end{lstlisting}

\paragraph{3-10} The best we could possibly do is linear time---we must check that all letters match somehow. Assign each character in the alphabet the $n$th prime number. Hash each string by multiplying the prime numbers corresponding to each character in the string. This takes $O(n)$ time on the length of the string. If two strings are anagrams of each other, their hash will be the same number by virtue of the Fundamental Theorem of Arithmetic.

\paragraph{3-11} Constant time search means \emph{random access}, so we will represent the dictionary using an array of length $n$. The universe of possible elements are the positive naturals up to and including $n$, so the absence of element $k$ can be signaled by setting the $k$th element of the backing array to 0.

\paragraph{3-12} Maximum depth of a tree:
\begin{lstlisting}
data Tree = Node Tree Tree
          | Nil

maxDepth :: Tree -> Int
maxDepth Nil = 0
maxDepth (Node l r) = 1 + max (maxDepth l) (maxDepth r)
\end{lstlisting}

\paragraph{3-14} Merging two binary search trees into a doubly linked list. The in order
traversal of each tree provides ordered lists of their respective elements. A linear
sweep, choosing the minimum of each head and advancing in the chosen list will merge
the two structures. The cost of both the traversal and sweep/merge stages is $O(m+n)$,
where $m$ and $n$ are the number of elements in each binary search tree.

Note that this is fundamentally the idea behind mergesort.

\paragraph{3-15} There is presumably an insertion order that guarantees height balance.
Knowing all the elements and having them in order is a great advantage. Some thoughts:
clearly the extremes have to be inserted among the last few elements--their levels have
to be created for them to land in the right place. Also, the median element of the array
has to be the root, as it gives the most ``space'' to its sides.

That median element determines two halves. Each half has its own median element to which
the same thinking process applies. Therefore a potential algorithm for building a height balanced tree is:
\begin{lstlisting}
balanced_insertion(lo, hi):
  m <- median(lo, hi)
  insert element m
  balanced_insertion(lo, m-1)
  balanced_insertion(m+1, hi)
\end{lstlisting}
With some provisions for ranges of length 1, in which case the element is a leaf and can
be inserted with no further recursive calls.

Before doing the insertion, the binary search tree has to be traversed in order to build
an array of all its elements. This takes $O(n)$ time.

Consider the call stack of \lstinline!balanced_insertion()! with the full range of $n$
elements. It makes two recursive calls, so the call stack is shaped like
a binary tree. The height of the call tree is at most $\lg n$, because the range of
elements in each recursive call is more than halved with respect to the range of its
caller. Then this tree can have at most $2^{1+\lg n} = 2\cdot 2^{\lg n} = 2n$ nodes, each
representing a call to \lstinline!balanced_insertion()!. Since each call does constant time work, this function's running time is $O(n)$ as well.

\paragraph{3-17} The definition of height balanced tree is recursive, but the recursive
aspect is implicit in the way it's worded in the book. An explicit way to say it is: a
binary tree is height balanced if both its children are height balanced and the difference
between their heights is at most 1. A node with no children is balanced and has height 1
(or, equivalently, a ``null'' node is balanced and has height 0).

\begin{lstlisting}
data Tree = Node Tree Tree
          | Nil

isBalanced :: Tree -> (Bool, Int)
isBalanced Nil = (True, 0)
isBalanced (Node l r) = (balanced, height)
  where (lb, lh) = isBalanced l
        (rb, rh) = isBalanced r
        height   = 1 + max lh rh
        balanced = lb && rb && abs (lh - rh) <= 1
\end{lstlisting}

Each call performs constant-time work, and there is only one call for each node in the
tree. Therefore this is an $O(n)$ time algorithm.

\paragraph{3-18} We have a balanced tree where all of \lstinline!search()!,
\lstinline!insert()!, \lstinline!delete()!, \lstinline!minimum()! and
\lstinline!maximum()! take $O(\log n)$ time, and we want to ensure it supports
\lstinline!successor()! and \lstinline!predecessor()! in $O(1)$ time. Each node will
have pointers to its logical predecessor and successor that will have to be
maintained during update operations. This would effectively add a doubly linked list
structure on top of the binary tree.

%\begin{forest}
%	[5
%		[3 [2 [1]] [4]]
%		[7 [6] [8]]]
%\end{forest}

\lstinline!successor()!: see pp. 85 in the book. The in order successor can be found, with
a parent pointer, by considering two different cases: if the node has a right subtree, and
if it hasn't.

\lstinline!predecessor()!: finding it is symmetric to the successor.
\begin{itemize}
	\item If a node is a leaf, and is the left child of its parent, traverse up until 
		finding a node that is the right child of its parent. The parent will be the 
		predecessor.
	\item If a node is a leaf and is a right child, this is the trivial case of the
		previous point---the parent is the predecessor.
	\item If a node has a left subtree, the predecessor is the maximum of that left
		subtree.
\end{itemize}

When deleting an item from the tree, we can get ahold of its successor and predecessor
in $O(1)$ time via the pointers, delete the item, then link the pointers as done in a
doubly linked list.

Note that the exercise says the tree is balanced. We can assume there is a balancing
operation that takes place after each insert and delete, before the successor and
predecessor pointers are updated.

\paragraph{3-19} We have a dictionary with $O(\log n)$ \lstinline!search()!,
\lstinline!insert()!, \lstinline!delete()!, \lstinline!min()!, \lstinline!max()!,
\lstinline!predecessor()! and \lstinline!successor()!, and we want to make some changes
to \lstinline!insert()! and \lstinline!delete()! so \lstinline!min()! and \lstinline!max()!
will take $O(1)$ time while the update operations still take $O(\log n)$ time.

After \lstinline!insert()! we can query for the new element's predecessor in $O(\log n)$
time---if there isn't one, our new element is the new minimum. Similarly with
\lstinline!maximum()! and \lstinline!successor()!.

Before \lstinline!delete()! we can query for the predecessor of the element---if there
isn't one we can find the new minimum by querying for its successor (although in reality
we'd know we are trying to delete the minimum because we have a $O(1)$ minimum); likewise
when deleting the maximum. If before delete we do find a more extreme element, we know
we are not deleting the minimum or maximum and no adjustment is necessary.

\paragraph{3-20} The exercise calls this structure a ``set'', and by its operations it
does look like a set, so we'll assume the elements are unique. Furthermore, we will assume
we have a balancing operation that keeps the tree height balanced in $O(\log n)$ time.

Our set structure is backed by a balanced binary search tree. The \lstinline!member()!
query searches for the element, taking $O(\log n)$ time. The \lstinline!insert()!
operation is the regular BST insert---if the key already exists, it is overwritten; this
also takes $O(\log n)$ time. Finally, \lstinline!delete()! has to find the $k$-th smallest
element. Assume our BST has $O(\log n)$ \lstinline!minimum()! and $O(1)$ \lstinline!successor()!,
as in the previous exercise. Then we need to query for the minimum element and then $k$
calls to \lstinline!successor()!, then one $O(\log n)$ delete and one $O(\log n)$
rebalance.

Unfortunately this doesn't work---consider always deleting the last element. After finding
the minimum, we always do $n$ calls to \lstinline!successor()!, turning this into a $O(n)$
algorithm in the worst case. \lstinline!:\!

\paragraph{3-21} This may be cheating somehow, but if the two sets are disjoint and all
keys in $S_1$ are less than every key in $S_2$, we can concatenate both trees by finding
the minimum element in $S_2$ and making the root of $S_1$ its left child. It would wreak
havoc on the balance of the tree, but rebalancing should take at most $O(n+m)$ time, for
$n$ elements in $S_1$ and $m$ elements in $S_2$. The traversal of $S_2$ to its
minimum would be $O(log m)$ and the concatenation---a simple matter of pointer
manipulation---would be constant time.

\paragraph{3-22} We're to design a data structure that supports \lstinline!insert()!
and \lstinline!median()! operations, both in $O(\log n)$ time. Inserting is easy enough,
but the median is an element relative to all other elements in the structure; it depends
on its logical position within the set of all the elements, so it's not clear that it
can be determined without somehow keeping track of counts.

Suppose the structure keeps two binary search trees, the median, and counters for the
elements in each tree. The elements in the left tree are less than the median, while
those on the right tree are greater than the median.

When inserting, we compare the element to the median to select which side to insert it
into. It's then inserted in the correct tree, taking $O(\log n)$ time (actually $\log m$,
with $m$ the number of elements on that side of the median). We increase the counter on
this side, and if the difference in elements between both sides is greater than 1, we
know we need to shift the median.

To rebalance the structure, suppose the left side has two more elements than the right
side. We can insert the median in the right tree in $O(\log n)$ time, then find and
delete the maximum from the left tree, also in $O(\log n)$ time, and set it as the new
median. The right counter increases by one, the left counter decreases by one, and we are
balanced again.

This actually gives $O(1)$ access to the median, so it is probably wrong for the exercise.

\paragraph{3-23} Let $p$ be some prefix. We wish to find all strings in a dictionary $D$ that start with $p$.

Begin by querying the dictionary for $p$. If the prefix is not in $D$, insert it and make a note of it. Now query for the prefix to get a pointer $x$ to it in $D$. Call \lstinline!successor(D,x)! repeatedly, printing all the strings that match the prefix, stopping at the first one that does not. Finally, if the prefix was not in the dictionary to begin with, delete it from $D$.

Comparing strings is not like comparing integers---it takes time linear on the length of the strings. For prefix matching, since strings in $D$ are at most $l$ characters long, comparisons will take $O(l)$ time in the worst case. So the initial query for $p$ does at most $\log n$ comparisons, each taking $O(l)$ time, so this takes $O(l\log n)$ time. Insertion, querying again and the potential delete at the end also take $O(l\log n)$ time. Each call to \lstinline!successor()! is $O(\log n)$, and there will be $m$ matching strings each verified by a $O(l)$ comparison, for a total of $O(ml\log n)$ time complexity.

\paragraph{3-24} An array is $k$-unique if no elements within a $k$-wide range are equal. For example $\langle1,2,3,2,4\rangle$ is not 2-unique, and $\langle2,0,1,3,5,0\rangle$ is 3-unique but not 4-unique. Note that if an array is not $k$-unique, then it's not $(k+1)$-unique. If an array is $k$-unique, then it is $(k-1)$-unique.

A binary tree with $k$ elements can be built in $O(k)$ time. So do this:
\begin{enumerate}[label=\arabic*)]
	\item Insert the first $k$ elements into the tree, while checking for duplicates. Each query is $O(\log k)$ as is each insertion, and there are $k$ of them so this is $O(k\log k)$.
	\item If no dupes were found, remove the first item in the array from the tree, leaving a tree with $k-1$ elements. This takes $O(\log k)$ time. \label{3-24-step2}
	\item Query for the $(k+1)$th element of the array---if it is in the tree, the array is not $k$-unique. Otherwise, insert the element into the tree and go back to step \ref{3-24-step2}, subsequently removing the lowest indexed element in the array from the tree.
	\item If the last element of the array is successfully inserted, this means we didn't find any duplicates within a $k$ element window, so the array is $k$-unique.
\end{enumerate}

There are at most $n$ $O(\log k)$ groups of operations as the array is traversed while querying, so this algorithm is $O(n\log k)$ in the worst case.

\paragraph{3-25} We can represent the bins as a binary search tree, where each node is an integer representing the available space in that bin. This will bring trouble with bins simultaneously having the same available space, but let's ignore that.

We would insert a weight by querying for the minimum in the binary tree in $O(\log n)$ time---if the weight fits, save the updated weight in a new node, delete the stale bin in $O(\log n)$ time and insert the updated node, again in $O(\log n)$ time. If the weight does not fit in the first bin returned, call \lstinline!successor()! until one is found that has enough room. This search takes $O(n\log n)$ time in the worst case, as we go through all the bins. If no bin has space, create and insert a new node with the current weight inside. The minimum number of bins at the end is the number of nodes in the tree.

\paragraph{3-26}
\begin{enumerate}[label=\alph*)]
	\item Make an $n$-by-$n$ array and populate it in $O(n^2)$ time with the minimum value in $x_i,\ldots,x_j$ for $i,j$ in $[1\ldots n]$. Then given $i$ and $j$, just return whatever is in the $i,j$ position of the matrix in $O(1)$.
	\item Consider an unsorted array and take the minimum, say $x_i$. This element partitions the array into a left side, from $x_1$ to $x_{i-1}$; the minimum itself, $x_i$; the right side, from $x_{i+1}$ to $x_n$.
	
	Now make the root of a tree by pairing up $(x_i, i)$, the minimum and its position. The left subtree's root node is the minimum in the range $[x_1\ldots x_{i-1}]$ and itself splits this range in two sides (plus itself). Likewise with the right subtree and the range $[x_{i+1}\ldots x_n]$.
	
	Given a query range, we start on the root of the tree. If $i$ is in the query range, we have our (global) minimum. Otherwise the range is entirely to the left or right of $i$. Traverse accordingly.

	By following this tree until we hit a minimum that is in the input interval we will find the minimum value in $O(\log n)$ steps. Since each node is its value plus its index this takes $O(n)$ space.	
\end{enumerate}

\paragraph{3-27} From doing a couple of experiments, it seems that given some $k$, using the oracle to test $f(S, k-x_i)$ for each $x_i\in S$ selects and eliminates the correct elements.

Suppose a subset $T=\{x_1, x_2, x_3\}$ is a solution for set $S=\{x_1, x_2, x_3, x_4\}$ and $k$. That means $k = x_1+x_2+x_3$, and implies that there are subsets of $S$ that add up to $k-x_1=x_2+x_3,\ k-x_2=x_1+x_3$, and $k-x_3=x_1+x_2$.

If $f(S, k-x_4)$ is 0, that means no selection of elements $x_j$ will fulfill $k-x_4=\sum_j x_j$, equivalently that no selection of numbers that contains $x_4$ will add up to $k$.

So we have gone over $S$ once and eliminated all the numbers that could never be in a solution. But there may be more than one solution---that is, there may be proper subsets of $T$ that add up to $k$. For example, for $S=\{1,3,8,9,10\}$ and $k=18$, there are two solutions, $\{1,8,9\}$ and $\{8,10\}$. In this example, the process of elimination would have left us with $T=\{1,8,9,10\}$.
\begin{align*}
	f(S-\{1\}, k-1) &= 1 \\
	f(S-\{3\}, k-3) &= f(\{1,8,9,10\}, 15) = 0.
\end{align*}
This says we wouldn't be able to get up to 15 to use 3 to get us to $k=18$.
\begin{align*}
	f(S-\{8\}, k-8) = 1 \\
	f(S-\{9\}, k-9) = 1 \\
	f(S-\{10\}, k-10) = 1.
\end{align*}
Out of this $O(n)$ verification we are left with $T=\{1,8,9,10\}$ but $\sum_T t_i = 28 \ne k$. Now let $m=k-1=17$, and let $U=T-\{1\}=\{8,9,10\}$. We do the same:
\begin{align*}
	f(U-\{8\}, m-8) &= f(\{9,10\}, 9) = 1 \\
	f(U-\{9\}, m-9) &= f(\{8,10\}, 8) = 1 \\
	f(U-\{10\}, m-10) &= f(\{8,9\}, 7) = 0.
\end{align*}
From this, in a second $O(n)$ run, we have determined that the subset $\{1,8,9\}$ adds up to $k=18$.

Why does step two work? We know for sure a solution will include any element of $T$, let's say, $x_i$. Then the task is finding $x_j\in T$ such that $k = x_i + \sum_j x_j \rightarrow k-x_i = \sum_j x_j$. We can use our oracle to find a subset of $T-\{x_i\}$ that adds up to $k-x_i$, thus completing a solution.

\paragraph{3-28} We can use a \emph{segment tree} to solve this. They support more general range queries than prefix sums, but we can treat those as range queries with $l=0$.

Each node in a segment tree represents a range within the array and carries the partial sum for that range, with the root node representing the total range of the array. The subtrees represent each half of the range, such that their union is equal to the range represented by the parent. The root node of each subtree holds its corresponding partial sum. Each leaf node represents a one element range; naturally, the partial sum for this is the element itself.

%\tikz \draw (0pt,0pt) -- (20pt,6pt);

To update an element, traverse to the leaf node, update it, and trace back the path modifying each node. Alternatively modify each node visited on the way to the leaf node.

For partial sum queries, traverse to the leftmost leaf and find the right end of the range, then treat it as a special case of a general range query with $l=0$. The general range query $\sum_{[l,r]} x$ is answered by traversing the tree and using the precomputed sums of the segments. For some node corresponding to the range $[tl,tr]$ there are three cases:

\begin{enumerate}
	\item The segment $[l,r]$ is the same as $[tl,tr]$. Just return the precomputed sum of the node.
	\item The query segment is entirely contained in the domain of the left or right child. The left child covers $[tl,tm]$ and the right child covers $[tm+1, tr]$, with $tm=(tl+tr)/2$. In this case we recurse to the correct subtree and execute the algorithm on it.
	\item The final case is when the query segment intersects the domains of both children. Two recursive calls are made, each for the sum query of the \emph{intersection} of the range query and the child's domain, and the results are combined.
\end{enumerate}

The segment tree can be represented as a heap-style, packed implicit tree. We need at most $4n$ nodes for an array of size $n$. The root at index 1, the left child of $k$ at $2k$, the right child at $2k+1$. The parent is at $\lfloor k/2\rfloor$.

\chapter{Sorting}

%\section*{Solutions}

\subsection*{Applications of Sorting: Numbers}

\paragraph{4-1} The Grinch wants the most unbalanced game possible. This is the same as asking to maximize the difference in total skill between the two teams.

In $O(n\log n)$, sort the player pool by skill and make each half of the result a team. Any swap between teams would bring a higher skilled player into the lower half, and a lower skilled player into the higher half, making the difference in total skill lower.

\paragraph{4-2}
\begin{enumerate}[label=(\alph*)]
	\item Unsorted array. Find $x, y$ that maximize $|x-y|$ in $O(n)$ time. In one sweep through the array, find the minimum and maximum values. These maximize the difference.
	\item Sorted array. Find $x, y$ that maximize $|x-y|$ in $O(1)$ time. These are the first and last elements of the array.
	\item Unsorted array. Find $x, y$ that minimize $|x-y|$, for $x\ne y$, in $O(n\log n)$ time. Sort the array in $O(n\log n)$ then do a linear pairwise search of consecutive values for the pair with the smallest difference in $O(n)$ time.
	\item Sorted array. Find $x, y$ that minimize $|x-y|$, for $x\ne y$, in $O(n)$ time. Do a pairwise comparison of every element with the next in $O(n)$ and find the consecutive pair with the smallest difference.
\end{enumerate}

\paragraph{4-3} What we want to do here is first sort the array in $O(n\log n)$ time. Then we want to pair up the biggest offender with the least problematic number, so the largest with the smallest, the second largest with the second smallest, and so on.
\begin{equation*}
	x_1 < x_2 < \ldots < x_{2n-1} < x_{2n} \quad\rightarrow\quad (x_1, x_{2n}), (x_2, x_{2n-1}), \ldots
\end{equation*}

Why does this work? Suppose in my pairing I find that $(x,y)$ are my maximum pair. By way of contradiction, I claim that there is a different partition of the $2n$ numbers whose maximum pair is less than $(x,y)$.

Let's say that $x<y$. The claim that a different partition does better implies that, whatever its maximum pair is, element $y$ in this hypothetical partition has to be paired up with an element $t$ such that $t<x$, otherwise the maximum pair could not be less than $x+y$. But the same logic applies to every element larger than $y$. They need to be paired up with elements smaller than $x$ certainly, or for any $z>y$ we'd have a pair $(z,x)$ where $z+x > x+y$ is larger than the claimed maximum.

Now suppose there are $i$ elements larger than $y$. In my original pairing, that means there are $i$ elements smaller than $x$. But the claim forced me to pair $y$ with one of those $i$ elements smaller than $x$, so there are now $i-1$ numbers available to pair with the $i$ numbers larger than $y$. We see that we won't be able to find pairs for every element that don't surpass our claimed maximum pair. The contradiction shows that our original pairing was indeed optimal, and the algorithm correct.

\paragraph{4-4} Keep three linked lists and pointers to the end of each. Iterate over the input pairs, appending each to the list corresponding to its color. By the pointers to the ends, this can be done in $O(1)$ time. Finally concatenate all three lists, which is again constant time.

\paragraph{4-5} $O(n)$ is the best we can hope for, since we need to at least visit every element in the input. By using a hash table with (amortized) $O(1)$ query and insertion we can check for each value's membership, set it to 1 on first sight, then increment it on subsequent visits to construct a frequency table of all values. There are $n$ of them, and constant work for each (query, increment, insert), so building the table is $O(n)$ time. Iterating over all the key-value pairs and selecting the maximum frequency is another $O(n)$ effort after which we have found the mode.

\paragraph{4-6} We can afford to sort one of the sets into a sorted array $A$ in $O(n\log n)$ time. It's now possible to query $A$ for membership of a number in $O(\log n)$ time by binary search. Then for each number $b$ in the other set, let $a = x-b$, and query $A$ for $a$. If $a\in A$ then we have a number $a = x-b$, which is to say $x = a+b$, and we can answer positively. If we test every number in the second set unsuccessfully, there is no pair that adds to $x$. In this (worst) case, we do $n$ binary searches for a total $O(n\log n)$ time.

\paragraph{4-7} We can sort the array in descending order, then do a linear sweep, counting how many elements are strictly greater than their (0-based) index. Sorting is $O(n\log n)$ and the linear sweep is, well, linear, so overall this algorithm is $O(n\log n)$.

\begin{lstlisting}
int h(std::vector<int>& papers) {
  auto xs = papers;
  std::sort(xs.begin(), xs.end(), std::greater<int>());
  for (int i=0; i < xs.size(); i++) {
    if (xs[i] <= i)
      return i;
  }
  return xs.size();
}
\end{lstlisting}

Starting with the values sorted in descending order, consider the array of differences between each value and its index. Then an alternative view of the algorithm above is that the $h$-index of an array $A$ is the count of elements in the difference array that are greater than 0.
\begin{align*}
	\langle 6,5,4,3,3 \rangle &\rightarrow \langle 6,4,2,0,-1 \rangle \\
	\langle 5,4,4,2,1,1,0 \rangle &\rightarrow \langle 5,3,2,-1,-3,-4,-6 \rangle \\
	\langle 0,0,0,0 \rangle &\rightarrow \langle 0,-1,-2,-3 \rangle
\end{align*}

\paragraph{4-8}
\begin{enumerate}[label=(\alph*)]
	\item Assuming customers don't double pay, there will be at most as many checks as there are bills. Let's sort both bills and checks by the customer id. Then treat each list as a queue, and consider each bill. If the check at the front of its queue is for this bill, this customer has paid; pop the front of both queues. If the check doesn't match the bill, this bill hasn't been paid---pop the bill and put it aside in the list of unpaid bills.
	
	Sorting the bills and checks is $O(n\log n)$, and the linear sweep is $O(n)$ for a total effort of $O(n\log n)$ time.
	
	\item Sort the list of books by publisher. This is $O(n\log n)$ for $n$ books in the library. Then for each publisher use binary search to find the boundaries of the corresponding block of published books in the sorted list. (See section 5.1.1 Counting Occurrences.)
	
	Each binary search is $O(\lg n)$ time and there are 30 publishers, which is a constant factor so the overall effort is $O(n\log n)$ with the sorting of all the books dominating.
	
	\item Don't sort anything. Make an empty set, then for each card push the name into the set. The size of the set after going over all the cards is the answer.
	
	If the set is implemented as a binary tree, each insert is $O(\log n)$ for $n$ total cards, meaning $n$ insertions, so $O(n\log n)$ total effort. If the set is implemented as a hash table instead, each insertion is $O(1)$ so the overall complexity is $O(n)$.
\end{enumerate}

\paragraph{4-9} To start simple, let's consider the case for $k=1$. We are then looking to answer if $k\in S$ in $O(\log n)$ time. This is a tall order unless we make the further assumption that $S$ comes in the form of an already sorted array, in which case we can do a binary search. Otherwise a linear search will take $O(n)$ time and sorting, $O(n\log n)$.

For $k=2$ we can already afford to sort the array in $O(n\log n)$. Let $x$ be some element in $S$ and consider $y=T-x$, then we want to see if $y\in S$. This can be done in $O(\log n)$ time since the array is already sorted and we know what we are looking for.

Thinking back to exercise 3-27, we had a similar scenario in which we were given an oracle that could answer a simplified version of this question: given $T$ it would answer if there was a subset of \emph{some} length that added up to it. Here we have no oracle, and we have a fixed number $k$ of elements with which to build a solution.

As in exercise 3-27, let's start by assuming we have a solution $\left\{ x_1,\ldots,x_k \right\}$, that is $T = \sum^k x_i$. This means that before we were done we found $x_k = T - \sum^{k-1}$ to complete a partial solution $\left\{ x_1,\ldots,x_{k-1}\right\}$. That is, we searched $S - \left\{x_1,\ldots,x_{k-1}\right\}$ for $x_k$; but note that we only know to search for a specific value on the last $x_i$, when we know exactly what value we are missing in our solution to get to $T$. This is the scenario above where $k=2$.

In selecting the $x_1,\ldots,x_{k-1}$ we don't search but try these values out. This is where the $n^{k-1}$ factor comes from---it's the combinations of $k-1$ elements for the first $k-1$ positions, for each of which we can perform an intentional binary search in $O(\log n)$ for the final piece of the solution. More specifically, we do a $O(\log n)$ search for the $k$th value for, at most, $n^{k-1}$ partial solutions of $k-1$ elements.

Suppose we have a method to generate the $\binom{n}{k-1}$ combinations out of the set $S$. Then the algorithm would look something like this:
\begin{lstlisting}
for C in combinations(S, k-1):
  if (T - sum(C)) in S-C:
    return true
return false
\end{lstlisting}

\paragraph{4-10}
\begin{enumerate}[label=(\alph*)]
	\item If $S$ is unsorted, we can sort it in $O(n\log n)$ time, then for each of the $n$ elements do a binary search for the difference with $x$. At most we do a binary search for all $n$ elements, each being $O(\log n)$, for a total $O(n\log n)$ effort.
	
	\item If the array is sorted, consider two pointers $l$ and $r$, pointing to the first and last elements of the array respectively. While they haven't crossed each other, compare the sum of the pointed elements to $x$---if the sum is greater than $x$, point $r$ to the next smaller element; if the sum is less than $x$, point $l$ to the next largest element. If the two pointers cross each other, then no two elements in the array add up to $x$.
	
	\begin{proof}Let us call $l, r$ the left and right pointers respectively. Suppose a given array and real number $x$ have a solution at indices $i, j$---that is, $x = x_i + x_j,\ i<j$.
	
	The pointers $l$ and $r$ only ever go in one direction, so we would fail to find a solution if $l>i$ or $r<j$ (taking some notational license).
	
	Suppose there is a solution, yet $l = i+1$. Then $l$ was incremented because $x_i + x_j < x$, which contradicts the assumption that our solution was the pair $(x_i, x_j)$. Note that $r>j$, since the array is ordered, would mean that $x_i+x_{j+1} > x$ since $x_j < x_{j+1}$. But this contradicts the fact that $l$ was incremented to $i+1$.
	
	By a similar argument, we can see that $r$ can't be less than $j$.
	
	\vskip 1em
	
	To drive the point home, consider the moment where $l=i-1$ and $r=j+1$. Either $x < x_{i-1} + x_{j+1}$ or $x > x_{i-1} + x_{j+1}$.
	
	If $x < x_{i-1} + x_{j+1}$, we will increment $l$ and have a pair $(x_i,x_{j+1})$ which must be greater than $x$, for $x = x_i + x_j$ and $x_j < x_{j+1}$. At this point the algorithm will bring $r$ in to find pair $(x_i, x_j)$.
	
	If $x > x_{i-1} + x_{j+1}$, conversely, we will see $r$ be brought in first to find pair $(x_{i-1}, x_j)$. By the same argument, $x_{i-1} + x_j < x$, so $l$ will be advanced to point to $x_i$, and again find the solution $(x_i,x_j)$.
	\end{proof}
\end{enumerate}

\paragraph{4-11} This is cheating, but assume you have a hash table with $O(1)$ insertion and update. Traverse the list, incrementing the count of keys already in the table, or inserting them with an initial value of 1 otherwise. Finally, traverse the hash table filtering the items that fulfill the predicate.

\subsection*{Applications of Sorting: Intervals and Sets}

\paragraph{4-12}
\begin{enumerate}[label=(\alph*)]
	\item Sort the two arrays in $O(n\log n)$ then do a linear sweep merging from each array, while skipping duplicates. The linear sweep is, well, linear, while skipping duplicates is constant time because we will insert each unique number the first time we see it, then see it again immediately in the other set. \label{ch4e12a}
	\item Just do the merging part of \ref{ch4e12a}.
\end{enumerate}

\paragraph{4-13} Consider the number of people at the party at any given time as a counter. Then each $a_i$ increments the counter while each $b_i$ decrements it. A trivial algorithm is to gather all $a_i$ and $b_i$ in an array. Suppose we have a discriminant in each data point such that we can tell that a timestamp is an entrance or a departure. Sort the array in $O(n\log n)$ then do a linear sweep executing the increases and decreases on a counter while keeping track of the maximum throughout.

\paragraph{4-14} Sort the array of intervals by their first component in $O(n\log n)$. Then do a linear sweep, looking at the first element and its immediate successor.

If they overlap, that is if $x_{i+1} \le y_i$, merge them and make this new larger interval the current interval. Repeat the process skipping both of the intervals just processed.

If the intervals did not overlap, we finished merging a group of intersecting intervals which is disjoint from the next one. Push the interval under consideration into the list and continue ahead considering the next interval.

Finally, once we reach the end of the list, push the last interval under consideration into the list.

\paragraph{4-15} Consider a scenario in which two intervals intersect but don't intersect at their extremes. Then we see that in a sequential ordering of extremes, two intervals starts before one ends. The serialization of the endpoints of the example in the book would be $10\uparrow,15\uparrow,20\uparrow,40\downarrow,50\uparrow,60\downarrow,70\downarrow,90\downarrow$, where a $\uparrow$ means an interval opens at that point and $\downarrow$ means that an interval closes at that point.
%TODO image here

Now consider a scenario where two endpoints meet, for example for $S=\{(10,20),(20,30)\}$. The serialization of endpoints of this set would be $10\uparrow,20\uparrow\downarrow,30\downarrow$, meaning that at point 20 an endpoint ends and another one begins.

There is one final case to consider, that of intervals that start and end at the same point. For example, a set $S=\{(x_0, x_1), (y_0, y_1)\}$ with $y_0=y_1$. The serialization of endpoints of this set would be $x_0\uparrow, y_0\updownarrow, x_1\downarrow$ assuming that $x_1 > y_0$.

It's not hard to see that with an ordering of endpoints like this it's possible to sweep from left to right, maintaining a counter of simultaneously running intervals that changes at the points where intervals start or end. The counter increases when an interval is opened, and decreases when one is closed. This requires a little extra thought for scenarios where intervals intersect at their endpoints, as well as when 1-point intervals are involved, but the general idea is simple.

We set things up so as to process ``groups of endpoints'', each group being a collection of all endpoints that meet at a specific coordinate.
%TODO image here

At the coordinate of each group, some number of intervals meet at an endpoint. These plus any other currently running intervals, represented by the running counter, make up the total number of intervals that intersect at that point. For each group, we know which endpoints are opening or closing intervals, and which are 1-point (let's call them \emph{singular}) intervals; these are the arrows above. Therefore, when processing a group, given a counter $i$, counts of opening, closing and singular endpoints $o, c, s$ respectively, and a current maximum $m$ of intersecting intervals already seen, we have that
\begin{gather*}
	i' = i - c + o \\
	x = c + o + s \\
	m' = \max(m, x)
\end{gather*}
Where $i'$ is the updated counter after processing this group, $x$ is the number of intersecting intervals at this group's coordinate, and $m'$ is the updated maximum after processing this group.

Two things to note. When updating the counter, the number $s$ is insignificant because these are intervals that open and close at a singular point. Thus the number of running intervals before and after this point will not be affected by them. On the other hand, when calculating the number of intersecting intervals $x$ at the current group's coordinate, the singular intervals are counted and the closing intervals are ignored, because these were already accounted for when they were opened.

Thus after processing these groups in order, we will have a final maximum carrying both a point (at some interval's endpoint) and a count of intervals that intersect at that point.

The code for this is too long to include here. See \lstinline!ex04_15.hs! in the Haskell directory for a full implementation.

\paragraph{4-16} Sort $S$ by the starting coordinate of each segment in $O(n\log n)$.

Start by finding the segment that covers 0 that extends furthest to the right in $O(n)$. If no segment covers 0 then there is no solution.

Call the selected segment the ``current'' segment. Then, while $m$ hasn't been covered, select among the intervals that intersect the current interval the one that extends furthest to the right ($O(n)$), and make it the current segment. If no segment intersects the current segment whose right coordinate extends further to the right than that of the current segment, then there is no solution (the union of $S$ either has a gap or can't cover all of $(0,m)$).

\vskip 0.5em

This process guarantees that a sequence of intersecting segments will be selected, meaning that there will be no gaps. The selection of each segment in that sequence, the one that reaches furthest to the right, is optimal: any alternative will reach at most the same number of segments to the right, or it will possibly not reach the next optimal choice of segment.

\vskip 0.5em

\begin{proof}[Deniz's reasoning.] Let us take $m=1$ without loss of generality.

Let $I_i = [a_i, b_i],\ i=1,\ldots,m$ be the collection of segments which the algorithm yields, which by construction has the property that $a_{j+1} \leq b_j < b_{j+1}$ for $j=1,\ldots,m-1$. 

Let $J_i = [x_i, y_i],\ i=1,\ldots,n$ be another collection which covers $[0,1]$ with $n < m$. We can assume the collection $\{J_i\}$ has the same property (just apply the algorithm to this collection).

Since $n<m$, we have $b_m < 1 \leq y_m$. Also, by construction, $b_1 \geq y_1$. Let $S$ be the set of indices $i$ such that $b_i < y_i$. Then $m\in S$ and $1\notin S$. Let $k = \min S > 1$. Then, since $k - 1 \notin S$, we have $y_{k-1} \leq b_{k-1}$, as well as $x_k \leq y_{k-1}$ by construction and $b_k < y_k$ because $k\in S$. Thus $x_k \leq y_{k-1} \leq b_{k-1} < b_k < y_k$. 

So, $J_k = [x_k, y_k]$ is a segment which intersects $I_{k-1}$, but again by construction, $b_k$ is the largest right endpoint of all such intervals, implying $b_k \geq y_k$. But that means $k\notin S$, a contradiction. It follows that $m \leq n$.
\end{proof}

\subsection*{Heaps}

\paragraph{4-17} Building the complete heap with the regular \lstinline!bubble_up()! method will require $O(n\log n)$ time, so that will not work. The key is the faster heap construction method that uses \lstinline!bubble_down()! instead. Even though this operation is ostensibly $O(\log n)$, a closer look shows that $O(n\log n)$ is a generous upper bound for the cost of building a heap.

Half of the elements in the array are leaves, and can be considered well formed heaps of height 0. A fourth of the elements are heaps of height 1. According to Skiena there are at most $\lceil n/2^{h+1}\rceil$ nodes of height $h$; the bottom line is that this quickly converges to linear behavior.

Thus we have a way of building a heap in $O(n)$ time. All that is left is to do $k$ queries for the minimum, $O(1)$ each, and $k$ delete operations, $O(\log n)$ each, for a total $O(n + k\log n)$ time.

\paragraph{4-18} To clarify, $n$ elements across all $k$ lists.

Put the $k$ heads of the lists in a heap in $O(k)$ time. Suppose we build this priority queue so that, upon eliminating a minimum that belonged to list $i$, it pulls the new head from list $i$ (if it's not empty yet) into the heap. Effectively, keep a $k$-element heap of the heads of the lists.

Repeatedly take the minimum from the heap until there are no more elements in any of the lists (in other words, when we have processed $n$ elements). We will have produced an ordered sequence that merges all the elements in the lists.

There are $n$ total elements, selected from the lists by the \lstinline!pop()! operation of the heap which is $O(\log k)$, for a total $O(n\log k)$ time.

\vskip 0.5em

The powerup comes from the assistance the heap provides in quickly finding the next element in the output sequence. Without it, we would have to visit each of the $k$ list heads to find the minimum at each step. We'd do this $O(k)$ work for each of the $n$ elements; this is the obvious $O(kn)$ algorithm.















\end{document}